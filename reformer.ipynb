{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Close', 'Máy đẹp, thêm hình được không thớt', 'Fix thêm hơm bạn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from tokenizer import get_files, get_threads\n",
    "\n",
    "files = get_files(\"./data\")\n",
    "threads = [x for x in get_threads(files)]\n",
    "threads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Combo Main Giga H81M-DS2 (có chặn)– CPU G3450: 800k (SL 6) Combo Main Biostar H61MGV3 (itx) – G2030 – Ram 2G : 600k Combo Main Asus P8H61 – CPU Celeron G550 – Ram 4G (2x2) : 650k Main lẻ Giga H81M-DS2 (không chặn): 500k (SL 3) hoặc trade main Asus H81M-D hoặc E GD HN (Khu vực cầu Nguyễn An Ninh hoặc Khuất duy Tiến) Hoặc Bank chuyển ship.. Main: giga Z390UD bh 13/06/2022: 1tr9 2 Chân màn dell 2312 150k/1. Mua cả 250k ĐC: 29 nguyễn bỉnh khiêm giờ hành chính sđt/zalo: 0948585179'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_convo(threads):\n",
    "#     SEP_TOKEN = \" <SEP> \"\n",
    "    SEP_TOKEN = \". \"\n",
    "    \n",
    "    for thread in threads:\n",
    "        if len(thread) < 2:\n",
    "            continue\n",
    "        yield SEP_TOKEN.join(thread)\n",
    "\n",
    "\n",
    "random.shuffle(threads)\n",
    "convo_generator = get_convo(threads)\n",
    "next(convo_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of threads: 205340\n"
     ]
    }
   ],
   "source": [
    "convo_generator = get_convo(threads)\n",
    "all_convo = [x for x in convo_generator]\n",
    "# all_convo = [x for thread in threads for x in thread if len(x) > 10]\n",
    "random.shuffle(all_convo)\n",
    "\n",
    "print(\"Number of threads:\", len(all_convo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of threads: 10000\n"
     ]
    }
   ],
   "source": [
    "all_convo = random.sample(all_convo, 10000)\n",
    "print(\"Number of threads:\", len(all_convo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"voz_tokenizer.json\",\n",
    "    unk_token=\"<UNK>\",\n",
    "    pad_token=\"<PAD>\",\n",
    "    bos_token=\"<BOS>\",\n",
    "    eos_token=\"<EOS>\",\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ['1m4.', '3c', 'gskill', 'ripjaws', 'Z', '4gb', '1600', 'cas9', '300/1', '2c']\n",
      "input_ids [8100, 24, 18, 23, 71, 75, 8913, 13975, 8503, 13966] ...\n",
      "token_type_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ...\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ...\n",
      "tokens ['1m', '4', '.', '3', 'c', 'g', 'skill', 'rip', 'ja', 'ws']\n"
     ]
    }
   ],
   "source": [
    "encodings = tokenizer(all_convo[0])\n",
    "\n",
    "print(\"input\", all_convo[0].split()[:10])\n",
    "for k,v in encodings.items():\n",
    "    print(k,v[:10],\"...\")\n",
    "print(\"tokens\", tokenizer.convert_ids_to_tokens(encodings.input_ids[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3002, 3160, 3407, 3685, 4066, 4753, 5185, 7272, 8202, 16337]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(tokenizer(x).input_ids) for x in random.sample(all_convo, 100)]\n",
    "lengths.sort()\n",
    "lengths[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = random.sample(all_convo, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 2**15\n",
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([7650, 6436, 5896,  ...,    1,    1,    1]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'labels': tensor([7650, 6436, 5896,  ...,    1,    1,    1])}\n",
      "{'input_ids': tensor([[ 7650,  6436,  5896,  ...,     1,     1,     1],\n",
      "        [ 6407,  6038,  6668,  ...,     1,     1,     1],\n",
      "        [ 7437,  6554,  5937,  ...,     1,     1,     1],\n",
      "        [11083,  5929,  5874,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 7650,  6436,  5896,  ...,     1,     1,     1],\n",
      "        [ 6407,  6038,  6668,  ...,     1,     1,     1],\n",
      "        [ 7437,  6554,  5937,  ...,     1,     1,     1],\n",
      "        [11083,  5929,  5874,  ...,     1,     1,     1]])}\n"
     ]
    }
   ],
   "source": [
    "class VozDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        encodings = self.tokenizer(\n",
    "            self.texts[i], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        encodings[\"labels\"] = encodings[\"input_ids\"]\n",
    "        return {k:torch.tensor(v) for k,v in encodings.items()}\n",
    "\n",
    "dataset = VozDataset(samples, tokenizer, sequence_length)\n",
    "\n",
    "print(dataset[0])\n",
    "print(dataset[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 7650,  6436,  5896,  ...,     1,     1,     1],\n",
       "         [ 6407,  6038,  6668,  ...,     1,     1,     1],\n",
       "         [ 7437,  6554,  5937,  ...,     1,     1,     1],\n",
       "         [11083,  5929,  5874,  ...,  6183, 10793,  6452]]),\n",
       " 'labels': tensor([[ 7650,  6436,  5896,  ...,     1,     1,     1],\n",
       "         [ 6407,  6038,  6668,  ...,     1,     1,     1],\n",
       "         [ 7437,  6554,  5937,  ...,     1,     1,     1],\n",
       "         [11083,  5929,  5874,  ...,  6183, 10793,  6452]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VozDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=float(\"inf\")):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        encodings = self.tokenizer(\n",
    "            self.texts[i], \n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        if len(encodings.input_ids) > self.max_length:\n",
    "            return encodings.input_ids[:self.max_length]\n",
    "        else:\n",
    "            return encodings.input_ids\n",
    "\n",
    "class VozCollator():\n",
    "    def __init__(self, align_by, max_length, pad_value=0):\n",
    "        self.align_by = align_by\n",
    "        self.max_length = max_length\n",
    "        self.pad_value = pad_value\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        length = max([len(x) for x in batch])\n",
    "        length = min(self.max_length, length)\n",
    "        length = np.ceil(length/self.align_by).astype(int) * self.align_by   # align to chunk size\n",
    "        \n",
    "        attention_mask = [[1]*len(x) + [0]*(length-len(x)) if len(x) < length else [1]*length for x in batch]\n",
    "        input_ids = [x + [self.pad_value]*(length-len(x)) if len(x) < length else x[:length] for x in batch]\n",
    "\n",
    "        output = {\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"labels\": torch.tensor(input_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "\n",
    "dataset = VozDataset(samples, tokenizer, max_length=sequence_length)\n",
    "collator = VozCollator(64, 1024, tokenizer.pad_token_id)\n",
    "batch = collator(dataset[:4])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train threads: 204340\n",
      "Test threads: 1000\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(all_convo)\n",
    "# train_size = int(len(all_convo)*0.95)\n",
    "train_size = len(all_convo) - 1000\n",
    "\n",
    "train_convo = all_convo[:train_size]\n",
    "test_convo = all_convo[train_size:]\n",
    "\n",
    "print(\"Train threads:\", len(train_convo))\n",
    "print(\"Test threads:\", len(test_convo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VozDataset(train_convo, tokenizer, sequence_length)\n",
    "test_dataset = VozDataset(test_convo, tokenizer, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://huggingface.co/transformers/master/model_doc/reformer.html#axial-positional-encodings\n",
    "\n",
    "<blockquote>\n",
    "In practice, the parameter config.axial_pos_embds_dim is set to a tuple (d1,d2) which sum has to be equal to config.hidden_size and config.axial_pos_shape is set to a tuple (n1s,n2s) which product has to be equal to config.max_embedding_size, which during training has to be equal to the sequence length of the input_ids.\n",
    "</blockquote>\n",
    "\n",
    "From https://huggingface.co/transformers/master/model_doc/reformer.html#training\n",
    "\n",
    "<blockquote>\n",
    "During training, we must ensure that the sequence length is set to a value that can be divided by the least common multiple of config.lsh_chunk_length and config.local_chunk_length and that the parameters of the Axial Positional Encodings are correctly set as described above. Reformer is very memory efficient so that the model can easily be trained on sequences as long as 64000 tokens.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ReformerModelWithLMHead, ReformerConfig\n",
    "\n",
    "config = {\n",
    "#     \"attention_head_size\": 64,\n",
    "#     \"attn_layers\": [\"local\", \"lsh\", \"local\", \"lsh\", \"local\", \"lsh\"],\n",
    "#     \"axial_pos_embds\": True,\n",
    "#     \"sinusoidal_pos_embds\": False,\n",
    "#     \"axial_pos_embds_dim\": [64, 192],\n",
    "    \"axial_pos_shape\": [2**7, 2**8],\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \n",
    "#     \"lsh_attn_chunk_length\": 64,\n",
    "#     \"local_attn_chunk_length\": 64,\n",
    "    \n",
    "#     \"feed_forward_size\": 512,\n",
    "#     \"hidden_act\": \"relu\",\n",
    "#     \"hidden_size\": 256,\n",
    "    \n",
    "    \"is_decoder\": True,\n",
    "    \"max_position_embeddings\": sequence_length,\n",
    "    \"num_attention_heads\": 2,\n",
    "#     \"num_buckets\": [64, 128],\n",
    "#     \"num_buckets\": None,\n",
    "#     \"num_hashes\": 1,\n",
    "    \"vocab_size\": VOCAB_SIZE,\n",
    "#     \"lsh_attention_probs_dropout_prob\": 0.0,\n",
    "#     \"lsh_num_chunks_before\": 1,\n",
    "#     \"lsh_num_chunks_after\": 0,\n",
    "#     \"local_num_chunks_before\": 1,\n",
    "#     \"local_num_chunks_after\": 0,\n",
    "#     \"local_attention_probs_dropout_prob\": 0.025,\n",
    "#     \"hidden_dropout_prob\": 0.025,\n",
    "}\n",
    "\n",
    "config = ReformerConfig(\n",
    "    axial_pos_embds=False,\n",
    "#     axial_pos_shape=[2**7, 2**8],\n",
    "    num_buckets=[32, 32],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    is_decoder=True,\n",
    "    max_position_embeddings=sequence_length,\n",
    "#     num_attention_heads=2,\n",
    ")\n",
    "model = ReformerModelWithLMHead(config)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    non_padded_indices = (pred.label_ids != -100)\n",
    "    \n",
    "    labels = pred.label_ids[..., 1:][non_padded_indices[..., 1:]]\n",
    "    pred = np.argmax(pred.predictions[:, :-1], axis=-1)[non_padded_indices[..., :-1]]\n",
    "    acc = np.mean(np.asarray(pred == labels), dtype=np.float)\n",
    "\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    num_train_epochs=5,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    \n",
    "    fp16=True,\n",
    "    fp16_opt_level=\"O2\",\n",
    "#     gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_accumulation_steps=1,\n",
    "#     prediction_loss_only=True,\n",
    "    \n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    save_steps=1000,\n",
    "    save_total_limit=10,\n",
    "    \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    \n",
    "#     logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/student11_5/miniconda3/envs/vn-chatbot/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2099' max='31930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2099/31930 26:09 < 6:12:09, 1.34 it/s, Epoch 0.33/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.322900</td>\n",
       "      <td>5.255658</td>\n",
       "      <td>0.436373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ReformerModelWithLMHead(config)\n",
    "model.train()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"voz_model\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xin chào tôi tên là Trần Nguyễn Mạnh Thiên. <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>.,..,... <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "output = model.generate(\n",
    "    **tokenizer(\"xin chào tôi tên là Trần Nguyễn Mạnh Thiên\", return_tensors=\"pt\").to(model.device),\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_k=10,\n",
    "#     bad_words_ids=[tokenizer(\"\").input_ids]\n",
    ")\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import trax\n",
    "\n",
    "output = trax.data.tokenize(iter([sample]), vocab_type=\"sentencepiece\", vocab_file=\"voz_out.model\", vocab_dir=\".\")\n",
    "output = list(output)\n",
    "\n",
    "restored = trax.data.detokenize(output[0], vocab_type=\"sentencepiece\", vocab_file=\"voz_out.model\", vocab_dir=\".\")\n",
    "\n",
    "print(output)\n",
    "print(restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = len(all_convo) // 20\n",
    "\n",
    "train_convo = all_convo[:-val_size]\n",
    "val_convo = all_convo[-val_size:]\n",
    "\n",
    "print(\"Train size:\", len(train_convo))\n",
    "print(\"Val size:\", len(val_convo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(data):\n",
    "    # loop over the entire data\n",
    "    while True:\n",
    "        # get a random element\n",
    "        d = random.choice(data)\n",
    "        \n",
    "        # yield a tuple pair of identical values \n",
    "        # (i.e. our inputs to the model will also be our targets during training)\n",
    "        yield (d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_DIR = \".\"\n",
    "VOCAB_FILE = \"voz_out.model\"\n",
    "\n",
    "# trax allows us to use combinators to generate our data pipeline\n",
    "data_pipeline = trax.data.Serial(\n",
    "    # randomize the stream\n",
    "    trax.data.Shuffle(),\n",
    "    \n",
    "    # tokenize the data\n",
    "    trax.data.Tokenize(\n",
    "        vocab_type=\"sentencepiece\",\n",
    "        vocab_dir=VOCAB_DIR,\n",
    "        vocab_file=VOCAB_FILE\n",
    "    ),\n",
    "    \n",
    "    # filter too long sequences\n",
    "    trax.data.FilterByLength(2048),\n",
    "    \n",
    "    # bucket by length\n",
    "    trax.data.BucketByLength(\n",
    "        boundaries=[32,128,512,2048],\n",
    "        batch_sizes=[256,64,16,4,1]\n",
    "    ),\n",
    "    \n",
    "    # add loss weights but do not add it to the padding tokens (i.e. 0)\n",
    "    trax.data.AddLossWeights(id_to_mask=0)\n",
    ")\n",
    "\n",
    "# apply the data pipeline to our train and eval sets\n",
    "train_stream = data_pipeline(stream(train_convo))\n",
    "eval_stream = data_pipeline(stream(val_convo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stream generators will yield (input, target, weights). let's just grab the input for inspection\n",
    "inp, _, _ = next(train_stream)\n",
    "\n",
    "# print the shape. format is (batch size, token length)\n",
    "print(\"input shape: \", inp.shape)\n",
    "\n",
    "# detokenize the first element\n",
    "print(trax.data.detokenize(inp[0], vocab_type=\"sentencepiece\", vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reformer(vocab_size=10000, n_layers=2, mode=\"train\", attention_type=trax.layers.SelfAttention):\n",
    "    model = trax.models.reformer.ReformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        n_layers=n_layers,\n",
    "        mode=mode,\n",
    "        attention_type=attention_type\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = get_reformer()\n",
    "print(model)\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = trax.lr.warmup_and_rsqrt_decay(\n",
    "    n_warmup_steps=1000, \n",
    "    max_value=0.01\n",
    ")\n",
    "\n",
    "train_task = trax.supervised.training.TrainTask(            \n",
    "    labeled_data=train_stream,\n",
    "    loss_layer=trax.layers.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam(0.01),\n",
    "    lr_schedule=lr_schedule,\n",
    "    n_steps_per_checkpoint=10\n",
    ")\n",
    "\n",
    "eval_task = trax.supervised.training.EvalTask(                      \n",
    "    labeled_data=eval_stream,\n",
    "    metrics=[trax.layers.CrossEntropyLoss(), trax.layers.Accuracy()]\n",
    ")\n",
    "\n",
    "loop = trax.supervised.training.Loop(\n",
    "    get_reformer(),\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    "    output_dir=\"./model/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(*args, **kwargs):\n",
    "    kwargs['predict_mem_len'] = 120\n",
    "    kwargs['predict_drop_len'] = 120\n",
    "    return trax.layers.SelfAttention(*args, **kwargs)\n",
    "\n",
    "model = get_reformer(\n",
    "    mode='predict',\n",
    "    attention_type=attention,\n",
    ")\n",
    "\n",
    "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
    "\n",
    "model.init_from_file(\n",
    "    './model/model.pkl.gz',\n",
    "    weights_only=True, \n",
    "    input_signature=shape11)\n",
    "\n",
    "STARTING_STATE = model.state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vn-chatbot]",
   "language": "python",
   "name": "conda-env-vn-chatbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
